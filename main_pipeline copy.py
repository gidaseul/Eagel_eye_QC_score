# main_pipeline.py

import os
import sys
import argparse
import yaml
import pandas as pd
from datetime import datetime

# .env íŒŒì¼ ë¡œë“œë¥¼ ìœ„í•´ python-dotenv ì„¤ì¹˜ í•„ìš” (pip install python-dotenv)
from dotenv import load_dotenv
import google.generativeai as genai

# ê° ë‹¨ê³„ë³„ë¡œ ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆì˜ ë©”ì¸ í•¨ìˆ˜ë¥¼ import
from Crawling.naver_crawler import run_naver_crawling
from Crawling.kakao_crawler import run_kakao_crawling
from QC_score.score_pipline import run_scoring_pipeline
CONFIG_ENV_PATH = ".config.env"
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv(dotenv_path=CONFIG_ENV_PATH)

def load_config(config_path: str) -> dict:
    """YAML ì„¤ì • íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"ê²½ê³ : ì„¤ì • íŒŒì¼ '{config_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì½”ë“œì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return {}
    except Exception as e:
        print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return {}

def setup_api_key():
    """
    .env íŒŒì¼ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ê³  ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("ì˜¤ë¥˜: GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.", file=sys.stderr)
        return False
    
    try:
        genai.configure(api_key=api_key)
        list(genai.list_models()) # ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ í‚¤ ìœ íš¨ì„± ê²€ì¦
        print("âœ… Google Gemini API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        print(f"ì˜¤ë¥˜: Gemini API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒì„¸ ì˜¤ë¥˜: {e}", file=sys.stderr)
        return False


def main():
    """
    ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì¡°ìœ¨í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
    ì„¤ì • ìš°ì„ ìˆœìœ„: CLI ì¸ì > ì„¤ì • íŒŒì¼ (config.yaml) > ì½”ë“œ ë‚´ ê¸°ë³¸ê°’
    """
    # --- 1. ì„¤ì • ë° CLI ì¸ì ì²˜ë¦¬ ---
    parser = argparse.ArgumentParser(description="Naver/Kakao Crawling and Scoring Pipeline")
    parser.add_argument('--config', default='config.yaml', help='ì‚¬ìš©í•  ì„¤ì • íŒŒì¼ì˜ ê²½ë¡œ')
    parser.add_argument('--stage', type=str, help="ì‹¤í–‰í•  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ('naver', 'kakao', 'full')")
    parser.add_argument('--threads', type=int, help='ì‹¤í–‰í•  ìŠ¤ë ˆë“œ ê°œìˆ˜ (1-3)')
    parser.add_argument('--show-browser', action='store_true', help='ì´ í”Œë˜ê·¸ ì„¤ì • ì‹œ í¬ë¡¤ë§ ë¸Œë¼ìš°ì € ì°½ì„ í‘œì‹œí•©ë‹ˆë‹¤.')
    parser.add_argument('--input-file', type=str, help="ì…ë ¥ CSV íŒŒì¼ ì´ë¦„ (data_dir ë‚´ ìœ„ì¹˜í•´ì•¼ í•¨)")
    
    args = parser.parse_args()
    config = load_config(args.config)

    # ì„¤ì •ê°’ ê²°ì • (ìš°ì„ ìˆœìœ„: CLI > config.yaml > ê¸°ë³¸ê°’)
    PIPELINE_STAGE = args.stage or config.get('pipeline_stage', 'full')
    NUM_THREADS = args.threads or config.get('num_threads', 3)
    HEADLESS_MODE = not args.show_browser if args.show_browser else config.get('headless_mode', True)
    SAVE_INTERVAL = config.get('save_interval', 100)
    
    DATA_DIR = config.get('data_dir', 'data')
    OUTPUT_DIR = config.get('output_dir', 'results')
    INPUT_CSV_NAME = args.input_file or config.get('input_csv_name', 'name_location_output.csv')
    INPUT_CSV_PATH = os.path.join(DATA_DIR, INPUT_CSV_NAME)

    # --- 2. ì´ˆê¸° ì„¤ì • ë° ìœ íš¨ì„± ê²€ì‚¬ ---
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    today = datetime.now().strftime('%Y%m%d')

    if not setup_api_key():
        sys.exit(1)

    if not 1 <= NUM_THREADS <= 3:
        print(f"ê²½ê³ : ìŠ¤ë ˆë“œ ê°œìˆ˜ëŠ” 1ì—ì„œ 3 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤. {NUM_THREADS} -> 3ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
        NUM_THREADS = 3

    # --- 3. íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì‹¤í–‰ ---
    print(f"\n===== íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ë‹¨ê³„: {PIPELINE_STAGE.upper()}) =====")
    try:
        # [ ë‹¨ê³„ 1: ë„¤ì´ë²„ í¬ë¡¤ë§ ]
        print(f"\nğŸš€ [STAGE: NAVER] ë„¤ì´ë²„ ì§€ë„ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ìŠ¤ë ˆë“œ 1ê°œì¼ ê²½ìš° ì¤‘ê°„ ì €ì¥ ë¹„í™œì„±í™”
        effective_save_interval = SAVE_INTERVAL if NUM_THREADS > 1 else 0
        
        naver_df = run_naver_crawling(
            csv_path=INPUT_CSV_PATH,
            num_threads=NUM_THREADS,
            headless_mode=HEADLESS_MODE,
            save_interval=effective_save_interval,
            output_dir=OUTPUT_DIR,
        )

        if naver_df.empty:
            print("âŒ ë„¤ì´ë²„ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ì–´ íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return

        # ë„¤ì´ë²„ í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ (ì¤‘ê°„ ì‚°ì¶œë¬¼)
        naver_output_path = os.path.join(OUTPUT_DIR, f"1_naver_crawled_{today}.csv")
        naver_df.to_csv(naver_output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ. ì¤‘ê°„ ê²°ê³¼ ì €ì¥: {naver_output_path}")

        if PIPELINE_STAGE == 'naver':
            print("\nğŸ‰ 'naver' ë‹¨ê³„ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return

        # [ ë‹¨ê³„ 2: ì¹´ì¹´ì˜¤ í¬ë¡¤ë§ ]
        print(f"\nğŸš€ [STAGE: KAKAO] ì¹´ì¹´ì˜¤ë§µ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        kakao_df = run_kakao_crawling(
            input_df=naver_df,
            max_threads=NUM_THREADS,
            headless=HEADLESS_MODE
        )
        
        kakao_output_path = os.path.join(OUTPUT_DIR, f"2_kakao_added_{today}.csv")
        kakao_df.to_csv(kakao_output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ì¹´ì¹´ì˜¤ í¬ë¡¤ë§ ì™„ë£Œ. ì¤‘ê°„ ê²°ê³¼ ì €ì¥: {kakao_output_path}")

        if PIPELINE_STAGE == 'kakao':
            print("\nğŸ‰ 'kakao' ë‹¨ê³„ê¹Œì§€ì˜ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
            
        # [ ë‹¨ê³„ 3: ì ìˆ˜ ì‚°ì • ]
        print(f"\nğŸš€ [STAGE: SCORING] ì ìˆ˜ ì‚°ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        # ë°ì´í„°í”„ë ˆì„ì„ scoring ëª¨ë“ˆì´ ìš”êµ¬í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
        final_data_list = run_scoring_pipeline(
            input_data=kakao_df.to_dict('records'),
            data_dir=DATA_DIR # ì ìˆ˜ ì‚°ì •ìš© ë°ì´í„°(json, csv)ê°€ ìˆëŠ” í´ë” ê²½ë¡œ ì „ë‹¬
        )
        final_df = pd.DataFrame(final_data_list)
        
        # ìµœì¢… ê²°ê³¼ë¬¼ ì €ì¥
        final_output_path = os.path.join(OUTPUT_DIR, f"3_final_scored_data_{today}.csv")
        final_df.to_csv(final_output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ì ìˆ˜ ì‚°ì • ì™„ë£Œ. ìµœì¢… ê²°ê³¼ ì €ì¥: {final_output_path}")
        
        print("\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"\nğŸ’¥ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", file=sys.stderr)
        # ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥ì„ ìœ„í•´ traceback import
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()