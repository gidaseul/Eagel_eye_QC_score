# main_pipeline.py

import os
import sys
import argparse
import yaml
import pandas as pd
from datetime import datetime
import ast
import re
import boto3
from typing import Set, Dict, List, Any

# .env íŒŒì¼ ë¡œë“œë¥¼ ìœ„í•´ python-dotenv ì„¤ì¹˜ í•„ìš” (pip install python-dotenv)
from dotenv import load_dotenv
import google.generativeai as genai

# ê° ë‹¨ê³„ë³„ë¡œ ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆì˜ ë©”ì¸ í•¨ìˆ˜ë¥¼ import
from Crawling.naver_crawler import run_naver_crawling
from Crawling.kakao_crawler import run_kakao_crawling
from QC_score.score_pipline import run_scoring_pipeline
from Crawling.utils.master_loader import load_ids_from_master_data

CONFIG_ENV_PATH = ".config.env"
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv(dotenv_path=CONFIG_ENV_PATH)

def load_config(config_path: str) -> dict:
    """YAML ì„¤ì • íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âŒ ê²½ê³ : ì„¤ì • íŒŒì¼ '{config_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì½”ë“œì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return {}
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        return {}

def setup_api_key():
    """
    .env íŒŒì¼ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ê³  ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("ì˜¤ë¥˜: GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.", file=sys.stderr)
        return False
    
    try:
        genai.configure(api_key=api_key)
        list(genai.list_models()) # ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ í‚¤ ìœ íš¨ì„± ê²€ì¦
        print("âœ… Google Gemini API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: Gemini API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒì„¸ ì˜¤ë¥˜: {e}", file=sys.stderr)
        return False

def ensure_list_or_dict(x):
    """ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë‚˜ ë”•ì…”ë„ˆë¦¬ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if isinstance(x, (list, dict)):
        return x
    if isinstance(x, str):
        try:
            val = ast.literal_eval(x)
            if isinstance(val, (list, dict)):
                return val
        except (ValueError, SyntaxError):
            pass
    return x

def save_data(df: pd.DataFrame, filename_base: str, mode: str):
    """ë°ì´í„°í”„ë ˆì„ì„ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. íŒŒì¼ í™•ì¥ìëŠ” ìë™ìœ¼ë¡œ ë¶™ìŠµë‹ˆë‹¤."""
    if df.empty:
        print(f"ê²½ê³ : ì €ì¥í•  ë°ì´í„°ê°€ ì—†ì–´ '{filename_base}' íŒŒì¼ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    df = df.copy()
    for col in ['menu_list', 'review_info', 'theme_mood', 'theme_topic', 'theme_purpose', 'review_category']:
        if col in df.columns:
            df[col] = df[col].apply(ensure_list_or_dict)
            
    try:
        if mode in ["csv", "both"]:
            df.to_csv(f"{filename_base}.csv", encoding="utf-8-sig", index=False)
        if mode in ["json", "both"]:
            df.to_json(f"{filename_base}.json", orient='records', force_ascii=False, indent=4)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {filename_base}. ì—ëŸ¬: {e}", file=sys.stderr)

def main():
    """
    ë‹¨ì¼ ê²€ìƒ‰ ì‘ì—…ì— ëŒ€í•œ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì¡°ìœ¨í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    # --- 1. ì„¤ì • ë° CLI ì¸ì ì²˜ë¦¬ ---
    parser = argparse.ArgumentParser(description="Naver/Kakao Crawling and Scoring Pipeline for a single query.")
    
    # [ìˆ˜ì •] ì…ë ¥ ë°©ì‹ì„ CSVì—ì„œ ì§ì ‘ ê²€ìƒ‰ì–´ì™€ ì¢Œí‘œë¡œ ë³€ê²½
    parser.add_argument('-q', '--query', type=str, required=True, help='í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (í•„ìˆ˜)')
    parser.add_argument('--lat', type=float, help='ê²€ìƒ‰ ê¸°ì¤€ì  ìœ„ë„ (ì„ íƒ)')
    parser.add_argument('--lon', type=float, help='ê²€ìƒ‰ ê¸°ì¤€ì  ê²½ë„ (ì„ íƒ)')

    # [ìœ ì§€] ê¸°íƒ€ ì‹¤í–‰ ì˜µì…˜ë“¤
    parser.add_argument('--config', default='config.yaml', help='ì‚¬ìš©í•  ì„¤ì • íŒŒì¼ì˜ ê²½ë¡œ')
    parser.add_argument('--stage', type=str, help="ì‹¤í–‰í•  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ('naver', 'kakao', 'full')")
    parser.add_argument('--threads', type=int, help='ì¹´ì¹´ì˜¤ í¬ë¡¤ë§ì— ì‚¬ìš©í•  ìŠ¤ë ˆë“œ ê°œìˆ˜')
    parser.add_argument('--show-browser', action='store_true', help='ì´ í”Œë˜ê·¸ ì„¤ì • ì‹œ í¬ë¡¤ë§ ë¸Œë¼ìš°ì € ì°½ì„ í‘œì‹œí•©ë‹ˆë‹¤.')
    parser.add_argument('--format', type=str, choices=['csv', 'json', 'both'], help="ìµœì¢… ê²°ê³¼ íŒŒì¼ ì €ì¥ í˜•ì‹")
    
    args = parser.parse_args()
    config = load_config(args.config)

    # ì„¤ì •ê°’ ê²°ì • (ìš°ì„ ìˆœìœ„: CLI > config.yaml > ê¸°ë³¸ê°’)
    PIPELINE_STAGE = args.stage or config.get('pipeline_stage', 'full')
    HEADLESS_MODE = not args.show_browser
    OUTPUT_FORMAT = args.format or config.get('output_format', 'both')
    DATA_DIR = config.get('data_dir', 'data')
    KAKAO_MAX_THREADS = args.threads or config.get('num_threads', 3)
    
    # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ì„¤ì •
    run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    # ê²€ìƒ‰ì–´ë¥¼ íŒŒì¼ ì´ë¦„ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì•ˆì „í•œ ë¬¸ìë¡œ ë³€í™˜
    safe_query_name = re.sub(r'[\\/*?:"<>|]', "", args.query)
    OUTPUT_DIR = os.path.join(config.get('output_dir', 'results'), f"{safe_query_name}_{run_timestamp}")

    # --- 2. ì´ˆê¸° ì„¤ì • ---
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    if not setup_api_key():
        sys.exit(1)

    # --- 3. íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì‹¤í–‰ ---
    print(f"\n===== íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ë‹¨ê³„: {PIPELINE_STAGE.upper()}, ê²€ìƒ‰ì–´: '{args.query}') =====")
    
    # â˜…â˜…â˜… ìˆ˜ì •ëœ ë¶€ë¶„: S3 ë¡œì§ì„ ë¡œì»¬ JSON ë¡œë” í˜¸ì¶œë¡œ ë³€ê²½ â˜…â˜…â˜…
    # config.yamlì—ì„œ JSON íŒŒì¼ ê²½ë¡œë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
    id_json_path = config.get('id_json_path') # ì˜ˆ: 'data/existing_naver_ids.json'
    
    # íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œ, ë¡œì»¬ JSON íŒŒì¼ì—ì„œ ID ëª©ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    if id_json_path:
        crawled_naver_ids = load_ids_from_master_data(id_json_path)
    else:
        print("ê²½ê³ : ID ëª©ë¡ JSON íŒŒì¼ ê²½ë¡œ(id_json_path)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„, ì„¸ì…˜ ë‚´ ì¤‘ë³µë§Œ í™•ì¸í•©ë‹ˆë‹¤.")
        crawled_naver_ids = set()
    
    
    current_df = pd.DataFrame()

    try:
        # [ ë‹¨ê³„ 1: ë„¤ì´ë²„ í¬ë¡¤ë§ ]
        if PIPELINE_STAGE in ['naver', 'kakao', 'full']:
            print(f"\nğŸš€ [STAGE: NAVER] ë„¤ì´ë²„ ì§€ë„ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # [ìˆ˜ì •] ìƒˆë¡œì›Œì§„ run_naver_crawling í•¨ìˆ˜ í˜¸ì¶œ
            current_df = run_naver_crawling(
                search_query=args.query,
                latitude=args.lat,
                longitude=args.lon,
                headless_mode=HEADLESS_MODE,
                output_dir=OUTPUT_DIR,
                existing_naver_ids=crawled_naver_ids
            )
            
            if current_df.empty:
                print("âŒ ë„¤ì´ë²„ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ì–´ íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤."); return

            # ìƒˆë¡œ ìˆ˜ì§‘ëœ IDë“¤ì„ ì „ì²´ ëª©ë¡ì— ì¶”ê°€
            crawled_naver_ids.update(set(current_df['naver_id'].dropna().unique()))
            print(f"âœ… ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ. ê³ ìœ  ê°€ê²Œ {len(crawled_naver_ids)}ê°œ ìˆ˜ì§‘.")
            
            save_data(current_df, os.path.join(OUTPUT_DIR, "1_naver_crawled"), OUTPUT_FORMAT)
            
            if PIPELINE_STAGE == 'naver':
                print("\nğŸ‰ 'naver' ë‹¨ê³„ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."); return

        # [ ë‹¨ê³„ 2: ì¹´ì¹´ì˜¤ í¬ë¡¤ë§ ]
        if PIPELINE_STAGE in ['kakao', 'full']:
            print(f"\nğŸš€ [STAGE: KAKAO] ì¹´ì¹´ì˜¤ë§µ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            current_df = run_kakao_crawling(input_df=current_df, max_threads=KAKAO_MAX_THREADS, headless=HEADLESS_MODE)
            save_data(current_df, os.path.join(OUTPUT_DIR, "2_kakao_added"), OUTPUT_FORMAT)
            print(f"âœ… ì¹´ì¹´ì˜¤ í¬ë¡¤ë§ ì™„ë£Œ.")
            
            if PIPELINE_STAGE == 'kakao':
                print("\nğŸ‰ 'kakao' ë‹¨ê³„ê¹Œì§€ì˜ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."); return

        # [ ë‹¨ê³„ 3: ì ìˆ˜ ì‚°ì • ]
        if PIPELINE_STAGE == 'full':
            print(f"\nğŸš€ [STAGE: SCORING] ì ìˆ˜ ì‚°ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            final_data_list = run_scoring_pipeline(input_data=current_df.to_dict('records'), data_dir=DATA_DIR)
            
            if not final_data_list:
                print("âŒ ì ìˆ˜ ì‚°ì • ì‹¤íŒ¨. ìµœì¢… íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•Šê³  íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤."); return
            
            final_df = pd.DataFrame(final_data_list)
            final_output_base = os.path.join(OUTPUT_DIR, "3_final_scored_data")
            save_data(final_df, final_output_base, OUTPUT_FORMAT)
            
            print(f"âœ… ì ìˆ˜ ì‚°ì • ì™„ë£Œ. ìµœì¢… ê²°ê³¼ê°€ '{OUTPUT_DIR}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print("\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()

