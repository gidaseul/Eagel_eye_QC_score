# api_server.py (ìµœì¢… ì™„ì„±ë³¸)

import os
import sys
import uuid
import re
import yaml
import pandas as pd
import traceback
import subprocess
from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel, Field
from typing import Dict, Optional
from datetime import datetime
import google.generativeai as genai
from dotenv import load_dotenv

# ê¸°ì¡´ì— ë§Œë“¤ì—ˆë˜ íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆë“¤ì„ importí•©ë‹ˆë‹¤.
# ì´ í•¨ìˆ˜ë“¤ì´ api_server.pyì™€ ê°™ì€ ë ˆë²¨ì˜ í´ë”ì— ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
from Crawling.naver_crawler import run_naver_crawling
from Crawling.kakao_crawler import run_kakao_crawling
from QC_score.score_pipline import run_scoring_pipeline

# --- 1. ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” ---
app = FastAPI(title="Store Data Pipeline API")

# ì„¤ì • íŒŒì¼ ë¡œë“œ
try:
    config = yaml.safe_load(open("config.yaml", 'r', encoding='utf-8'))
except FileNotFoundError:
    print("ì˜¤ë¥˜: config.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    config = {}

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv(dotenv_path=".config.env")

# ì‘ì—… ìƒíƒœ ë° ê²°ê³¼ë¥¼ ì €ì¥í•  ì¸ë©”ëª¨ë¦¬ DB
tasks_db: Dict[str, Dict] = {}
# ì„œë²„ ì„¸ì…˜ ë™ì•ˆ ì¤‘ë³µ IDë¥¼ ê´€ë¦¬í•  set (ì„œë²„ ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”ë¨)
CRAWLED_IDS_IN_SESSION: set = set()

# --- 2. API ë°ì´í„° í˜•ì‹ ì •ì˜ (Pydantic) ---
class PipelineRequest(BaseModel):
    query: str = Field(..., description="í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (í•„ìˆ˜)", example="ì„±ìˆ˜ë™ ì¹´í˜")
    latitude: Optional[float] = Field(None, description="ê²€ìƒ‰ ê¸°ì¤€ì  ìœ„ë„ (ì„ íƒ)", example=37.544)
    longitude: Optional[float] = Field(None, description="ê²€ìƒ‰ ê¸°ì¤€ì  ê²½ë„ (ì„ íƒ)", example=127.044)
    show_browser: bool = Field(False, description="í¬ë¡¤ë§ ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ ì—¬ë¶€ (ë””ë²„ê¹…ìš©)")

class TaskResponse(BaseModel):
    task_id: str
    message: str

class StatusResponse(BaseModel):
    task_id: str
    status: str
    result_path: Optional[str] = None
    error: Optional[str] = None

# --- 3. í•µì‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def execute_pipeline_task(task_id: str, request: PipelineRequest):
    """ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¡œì§ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ìš©)"""
    output_dir = ""
    try:
        tasks_db[task_id] = {"status": "processing", "result_path": None, "error": None}
        print(f"[{task_id}] íŒŒì´í”„ë¼ì¸ ì‹œì‘: query='{request.query}'")

        safe_query_name = re.sub(r'[\\/*?:"<>|]', "", request.query)
        run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = os.path.join(config.get('output_dir', 'results'), f"{safe_query_name}_{task_id[:3]}_{run_timestamp}")
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. Naver Crawling
        tasks_db[task_id]["status"] = "processing: naver crawling"
        naver_df = run_naver_crawling(
            search_query=request.query,
            latitude=request.latitude,
            longitude=request.longitude,
            headless_mode=(not request.show_browser),
            output_dir=output_dir,
            existing_naver_ids=CRAWLED_IDS_IN_SESSION
        )
        if naver_df.empty: raise ValueError("ë„¤ì´ë²„ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        newly_crawled_ids = set(naver_df['naver_id'].dropna().unique())
        CRAWLED_IDS_IN_SESSION.update(newly_crawled_ids)
        print(f"[{task_id}] ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ. í˜„ì¬ê¹Œì§€ ëˆ„ì  ID ê°œìˆ˜: {len(CRAWLED_IDS_IN_SESSION)}")

        # 2. Kakao Crawling
        tasks_db[task_id]["status"] = "processing: kakao crawling"
        kakao_df = run_kakao_crawling(
            input_df=naver_df,
            max_threads=config.get('num_threads', 3),
            headless=(not request.show_browser)
        )

        # 3. Scoring
        tasks_db[task_id]["status"] = "processing: scoring"
        final_list = run_scoring_pipeline(
            input_data=kakao_df.to_dict('records'),
            data_dir=config.get('data_dir', 'data')
        )
        if not final_list: raise ValueError("ì ìˆ˜ ì‚°ì • ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        final_df = pd.DataFrame(final_list)
        final_output_path = os.path.join(output_dir, "final_result.json")
        final_df.to_json(final_output_path, orient='records', force_ascii=False, indent=2)
        
        tasks_db[task_id].update({"status": "completed", "result_path": final_output_path})
        print(f"[{task_id}] íŒŒì´í”„ë¼ì¸ ì„±ê³µ. ê²°ê³¼: {final_output_path}")

    except Exception as e:
        error_message = str(e)
        tasks_db[task_id].update({"status": "failed", "error": error_message})
        print(f"[{task_id}] íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {error_message}")
        if output_dir:
            error_log_path = os.path.join(output_dir, "error_log.txt")
            with open(error_log_path, 'w', encoding='utf-8') as f:
                f.write(f"Error: {error_message}\n\n")
                f.write(traceback.format_exc())

# --- 4. ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰ë  ì´ë²¤íŠ¸ ---
def clean_firefox_cache():
    """Firefox ê´€ë ¨ ìºì‹œ íŒŒì¼ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    try:
        print("ğŸ§¹ /tmp ë‚´ Firefox ìºì‹œ íŒŒì¼ ì •ë¦¬ ì‹œë„...")
        if sys.platform != "win32": # ìœˆë„ìš°ê°€ ì•„ë‹ ê²½ìš°ì—ë§Œ ì‹¤í–‰
            subprocess.run("rm -rf /tmp/rust_mozprofile* /tmp/Temp-*profile /tmp/geckodriver*", shell=True, check=False)
            print("âœ… ìºì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ.")
        else:
            print("- ìœˆë„ìš° í™˜ê²½ì—ì„œëŠ” ìºì‹œ ì •ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ìºì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def setup_api_key():
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("ê²½ê³ : GOOGLE_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Scoring ë‹¨ê³„ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    genai.configure(api_key=api_key)
    print("âœ… Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

@app.on_event("startup")
def on_startup():
    print("ğŸš€ API ì„œë²„ ì‹œì‘...")
    setup_api_key()
    clean_firefox_cache()

# --- 5. API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„ ---
@app.post("/pipeline/run", response_model=TaskResponse, status_code=202)
async def start_pipeline_endpoint(request: PipelineRequest, background_tasks: BackgroundTasks):
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ìš”ì²­í•˜ê³  ì¦‰ì‹œ ì‘ì—… IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    task_id = str(uuid.uuid4())
    tasks_db[task_id] = {"status": "pending"}
    
    background_tasks.add_task(execute_pipeline_task, task_id, request)
    
    return {"task_id": task_id, "message": "íŒŒì´í”„ë¼ì¸ ì‘ì—…ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤. '/pipeline/status/{task_id}'ë¡œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."}

@app.get("/pipeline/status/{task_id}", response_model=StatusResponse)
async def get_task_status_endpoint(task_id: str):
    """ì£¼ì–´ì§„ ì‘ì—… IDì˜ í˜„ì¬ ìƒíƒœì™€ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    task = tasks_db.get(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="í•´ë‹¹ ì‘ì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {"task_id": task_id, **task}