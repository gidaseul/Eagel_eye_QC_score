# api_server.py (ìµœì¢… ì™„ì„±ë³¸)

import os
import sys
import uuid
import re
import yaml
import pandas as pd
import traceback
import subprocess
from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel, Field
from typing import Dict, Optional, Any
from datetime import datetime
import google.generativeai as genai
from dotenv import load_dotenv
import boto3


# ê¸°ì¡´ì— ë§Œë“¤ì—ˆë˜ íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆë“¤ì„ importí•©ë‹ˆë‹¤.
# ì´ í•¨ìˆ˜ë“¤ì´ api_server.pyì™€ ê°™ì€ ë ˆë²¨ì˜ í´ë”ì— ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
from Crawling.naver_crawler import run_naver_crawling
from Crawling.kakao_crawler import run_kakao_crawling
from QC_score.score_pipline import run_scoring_pipeline
from Crawling.utils.master_loader import load_ids_from_master_data
from batch_consolidate import run_consolidation_job # ë°°ì¹˜ ì‘ì—… í•¨ìˆ˜ import
from copy import deepcopy

# --- 1. ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” ---
app = FastAPI(title="Store Data Pipeline API")
CONSOLIDATION_IN_PROGRESS = False # í†µí•© ì‘ì—… ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ í”Œë˜ê·¸


# ì„¤ì • íŒŒì¼ ë¡œë“œ
try:
    config = yaml.safe_load(open("config.yaml", 'r', encoding='utf-8'))
    STORAGE_MODE = config.get('storage_mode', 'local') # ì „ì—­ì ìœ¼ë¡œ ëª¨ë“œ ì €ì¥
except FileNotFoundError:
    print("ì˜¤ë¥˜: config.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    config = {}

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv(dotenv_path=".config.env")

# ì‘ì—… ìƒíƒœ ë° ê²°ê³¼ë¥¼ ì €ì¥í•  ì¸ë©”ëª¨ë¦¬ DB
tasks_db: Dict[str, Dict] = {}
# ì„œë²„ ì„¸ì…˜ ë™ì•ˆ ì¤‘ë³µ IDë¥¼ ê´€ë¦¬í•  set (ì„œë²„ ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”ë¨)
CRAWLED_IDS_IN_SESSION: set = set()


# --- 2. API ë°ì´í„° í˜•ì‹ ì •ì˜ (Pydantic) ---
class PipelineRequest(BaseModel): # ì…ë ¥ ê°’
    storage_mode: str = Field(
        default=STORAGE_MODE,
        description="ìŠ¤í† ë¦¬ì§€ ëª¨ë“œ (ë¡œì»¬ ë˜ëŠ” S3)",
        example="local or s3"
    )
    query: str = Field(..., description="í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (í•„ìˆ˜)", example="ì„±ìˆ˜ë™ ì¹´í˜")
    latitude: Optional[float] = Field(None, description="ê²€ìƒ‰ ê¸°ì¤€ì  ìœ„ë„ (ì„ íƒ)", example=37.544)
    longitude: Optional[float] = Field(None, description="ê²€ìƒ‰ ê¸°ì¤€ì  ê²½ë„ (ì„ íƒ)", example=127.044)
    zoom_level: Optional[int] = Field(None, description="ì§€ë„ í™•ëŒ€ ë ˆë²¨(ê¸°ë³¸ ê°’ 15) (ì„ íƒ)") # [ì‹ ê·œ] zoom_level í•„ë“œ ì¶”ê°€
    show_browser: bool = Field(False, description="í¬ë¡¤ë§ ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ ì—¬ë¶€ (ë””ë²„ê¹…ìš©)")

class TaskResponse(BaseModel): # ì‘ì—… ì‘ë‹µ í˜•ì‹
    task_id: str
    message: str

class StatusResponse(BaseModel):
    task_id: str
    status: str
    request_details: Optional[Dict[str, Any]] = Field(None, description="ìµœì´ˆ ìš”ì²­ íŒŒë¼ë¯¸í„°")
    # â˜…â˜…â˜… ë‘ ê°€ì§€ ê²½ë¡œë¥¼ ëª¨ë‘ ì˜µì…˜ìœ¼ë¡œ í¬í•¨ -> ë¡œì»¬ ëª¨ë“œì™€ S3 ëª¨ë“œ ì§€ì›
    result_path: Optional[str] = Field(None, description="[ë¡œì»¬ ëª¨ë“œ] ê²°ê³¼ íŒŒì¼ì´ ì €ì¥ëœ ë¡œì»¬ ê²½ë¡œ")
    result_url: Optional[str] = Field(None, description="[S3 ëª¨ë“œ] ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ì„ì‹œ URL")
    error: Optional[str] = None

# --- 3. í•µì‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def execute_pipeline_task(task_id: str, request: PipelineRequest, existing_ids: set):
    """ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¡œì§ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ìš©)"""
    output_dir = ""
    try:
        tasks_db[task_id] = {
            "status": "processing: starting",
            "request_details": request.model_dump()
        }
        print(f"[{task_id}] íŒŒì´í”„ë¼ì¸ ì‹œì‘: query='{request.query}'")
        
        # 1. Naver Crawling
        tasks_db[task_id]["status"] = "processing: naver crawling"
        naver_df = run_naver_crawling(
            search_query=request.query,
            latitude=request.latitude,
            longitude=request.longitude,
            headless_mode=(not request.show_browser),
            zoom_level=request.zoom_level,
            existing_naver_ids=existing_ids
        )
        if naver_df.empty: raise ValueError("ë„¤ì´ë²„ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        print(f"[{task_id}] ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ.")

        # 2. Kakao Crawling
        tasks_db[task_id]["status"] = "processing: kakao crawling"
        kakao_df = run_kakao_crawling(
            input_df=naver_df,
            max_threads=config.get('num_threads', 3),
            headless=(not request.show_browser)
        )

        # 3. Scoring
        tasks_db[task_id]["status"] = "processing: scoring"
        final_list = run_scoring_pipeline(
            input_data=kakao_df.to_dict('records'),
            data_dir=config.get('data_dir', 'data')
        )
        if not final_list: raise ValueError("ì ìˆ˜ ì‚°ì • ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        final_df = pd.DataFrame(final_list)
        
        # --- ê³µí†µ íŒŒì¼ëª… ë° ê²½ë¡œ ìƒì„± ---
        now = datetime.now()
        timestamp = now.strftime('%Y%m%d_%H%M%S')
        safe_query_name = re.sub(r'[\\/*?:"<>|]', "", request.query)
        file_name = f"{safe_query_name}_{timestamp}_{task_id[:8]}_{len(final_df)}.json"

        # -- ìŠ¤í† ë¦¬ì§€ ëª¨ë“œì— ë”°ë¥¸ ê²°ê³¼ ì €ì¥ ë¶„ê¸° -- # 
        if request.storage_mode == "s3":
            s3_config = config['s3_config']
            s3_client = boto3.client('s3')
            date_path = now.strftime('%Y-%m/%Y-%m-%d')
            final_s3_key = f"{s3_config['output_results_prefix']}{date_path}/{file_name}"

            json_buffer = final_df.to_json(orient='records', force_ascii=False, indent=4)
            s3_client.put_object(Bucket=s3_config['bucket_name'], Key=final_s3_key, Body=json_buffer, ContentType='application/json')
            
            tasks_db[task_id].update({"status": "completed", "s3_key": final_s3_key})
            print(f"[{task_id}] S3ì— ê°œë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {final_s3_key}")

        # local ëª¨ë“œì¼ ê²½ìš°
        else:
            local_config = config['local_config']
            date_path = now.strftime(os.path.join('%Y-%m', '%Y-%m-%d'))
            output_path = os.path.join(local_config['output_dir'], date_path)
            os.makedirs(output_path, exist_ok=True)
            
            final_local_path = os.path.join(output_path, file_name)
            final_df.to_json(final_local_path, orient='records', force_ascii=False, indent=4)
            
            tasks_db[task_id].update({"status": "completed", "result_path": final_local_path})
            print(f"[{task_id}] ë¡œì»¬ì— ê°œë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {final_local_path}")

    except Exception as e:
        error_message = str(e)
        tasks_db[task_id].update({"status": "failed", "error": error_message})
        print(f"[{task_id}] íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {error_message}")
        if output_dir:
            error_log_path = os.path.join(output_dir, "error_log.txt")
            with open(error_log_path, 'w', encoding='utf-8') as f:
                f.write(f"Error: {error_message}\n\n")
                f.write(traceback.format_exc())

# --- 4. ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰ë  ì´ë²¤íŠ¸ ---
def clean_firefox_cache():
    """Firefox ê´€ë ¨ ìºì‹œ íŒŒì¼ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    try:
        print("ğŸ§¹ /tmp ë‚´ Firefox ìºì‹œ íŒŒì¼ ì •ë¦¬ ì‹œë„...")
        if sys.platform != "win32": # ìœˆë„ìš°ê°€ ì•„ë‹ ê²½ìš°ì—ë§Œ ì‹¤í–‰
            subprocess.run("rm -rf /tmp/rust_mozprofile* /tmp/Temp-*profile /tmp/geckodriver*", shell=True, check=False)
            print("âœ… ìºì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ.")
        else:
            print("- ìœˆë„ìš° í™˜ê²½ì—ì„œëŠ” ìºì‹œ ì •ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ìºì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def setup_api_key():
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("ê²½ê³ : GOOGLE_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Scoring ë‹¨ê³„ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    genai.configure(api_key=api_key)
    print("âœ… Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")


@app.on_event("startup")
def on_startup():
    print(f"ğŸš€ API ì„œë²„ ì‹œì‘... (ìŠ¤í† ë¦¬ì§€ ëª¨ë“œ: {STORAGE_MODE.upper()})")
    setup_api_key()
    clean_firefox_cache()

# --- 5. API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„ --- # ì´ê±° ì–´ë–»ê²Œ post ë„˜ê²¨ì„œ ê°’ ë°›ì„ ì§€ ë‹¤ì‹œ ì •í•˜ê¸°
@app.post("/pipeline/run", response_model=TaskResponse, status_code=202)
async def start_pipeline_endpoint(request: PipelineRequest, background_tasks: BackgroundTasks):
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ìš”ì²­í•˜ê³  ì¦‰ì‹œ ì‘ì—… IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    task_id = str(uuid.uuid4())
    tasks_db[task_id] = {"status": "pending"}

    print(f"[{task_id}] ê¸°ì¡´ ë§ˆìŠ¤í„° ë°ì´í„°ì—ì„œ naver_id ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
    existing_ids = load_ids_from_master_data(request.storage_mode, config)

    background_tasks.add_task(execute_pipeline_task, task_id, request, existing_ids)

    return {"task_id": task_id, "message": "íŒŒì´í”„ë¼ì¸ ì‘ì—…ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤. '/pipeline/status/{task_id}'ë¡œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."}


@app.get("/config", response_model=dict)
async def get_config():
    """ì„œë²„ì˜ í˜„ì¬ ì£¼ìš” ì„¤ì •ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    # ë¯¼ê° ì •ë³´ë¥¼ ì œì™¸í•˜ê³  í˜„ì¬ í™œì„±í™”ëœ ëª¨ë“œì˜ ì„¤ì •ë§Œ ë³´ì—¬ì£¼ë„ë¡ ê°œì„ 
    active_config = config.get('local_config') if STORAGE_MODE == 'local' else config.get('s3_config')
    
    return {
        "storage_mode": STORAGE_MODE,
        "active_config": active_config
    }

# --- 6. ë°ì´í„° í†µí•© ìˆ˜ë™ ì‹¤í–‰ API ---
def consolidation_task_wrapper():
    """ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ í›„ ì ê¸ˆ í”Œë˜ê·¸ë¥¼ í•´ì œí•˜ëŠ” ë˜í¼ í•¨ìˆ˜."""
    global CONSOLIDATION_IN_PROGRESS
    try:
        run_consolidation_job()
    finally:
        CONSOLIDATION_IN_PROGRESS = False
        print("ë°ì´í„° í†µí•© ì‘ì—… ì™„ë£Œ. ì´ì œ ë‹¤ìŒ í†µí•© ìš”ì²­ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

@app.post("/admin/run-consolidation", response_model=TaskResponse, status_code=202)
async def trigger_consolidation_endpoint(background_tasks: BackgroundTasks):
    """ë°ì´í„° í†µí•© ë°°ì¹˜ ì‘ì—…ì„ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰ì‹œí‚µë‹ˆë‹¤. (ê´€ë¦¬ììš©)"""
    #  í•¨ìˆ˜ ë‚´ì— 'global' í‚¤ì›Œë“œ ì„ ì–¸ ì¶”ê°€ 
    global CONSOLIDATION_IN_PROGRESS
    
    if CONSOLIDATION_IN_PROGRESS:
        raise HTTPException(
            status_code=409,
            detail="ë°ì´í„° í†µí•© ì‘ì—…ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )

    CONSOLIDATION_IN_PROGRESS = True
    print("ê´€ë¦¬ì ìš”ì²­ìœ¼ë¡œ ë°ì´í„° í†µí•© ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.")
    background_tasks.add_task(consolidation_task_wrapper)
    
    return {"task_id": "consolidation_job", "message": "ë°ì´í„° í†µí•© ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."}
