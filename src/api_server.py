# api_server.py (ìµœì¢… ì™„ì„±ë³¸)

import os
import sys
import uuid
import re
import yaml
import pandas as pd
import traceback
import subprocess
from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel, Field
from typing import Dict, Optional, Any
from datetime import datetime
import google.generativeai as genai
from dotenv import load_dotenv
import boto3


# ê¸°ì¡´ì— ë§Œë“¤ì—ˆë˜ íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆë“¤ì„ importí•©ë‹ˆë‹¤.
from Crawling.naver_crawler import run_naver_crawling, run_target_naver_crawling
from Crawling.kakao_crawler import run_kakao_crawling
from QC_score.score_pipline import run_scoring_pipeline
from Crawling.utils.master_loader import load_ids_from_master_data
from batch_consolidate import run_consolidation_job # ë°°ì¹˜ ì‘ì—… í•¨ìˆ˜ import
from copy import deepcopy

# --- 1. ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” ---
app = FastAPI(title="Store Data Pipeline API")
CONSOLIDATION_IN_PROGRESS = False # í†µí•© ì‘ì—… ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ í”Œë˜ê·¸


# ì„¤ì • íŒŒì¼ ë¡œë“œ
try:
    config = yaml.safe_load(open("config.yaml", 'r', encoding='utf-8'))
    STORAGE_MODE = config.get('storage_mode', 'local') # ì „ì—­ì ìœ¼ë¡œ ëª¨ë“œ ì €ì¥
except FileNotFoundError:
    print("ì˜¤ë¥˜: config.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    config = {}

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv(dotenv_path=".config.env")

# ì‘ì—… ìƒíƒœ ë° ê²°ê³¼ë¥¼ ì €ì¥í•  ì¸ë©”ëª¨ë¦¬ DB
tasks_db: Dict[str, Dict] = {}
# ì„œë²„ ì„¸ì…˜ ë™ì•ˆ ì¤‘ë³µ IDë¥¼ ê´€ë¦¬í•  set (ì„œë²„ ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”ë¨)
CRAWLED_IDS_IN_SESSION: set = set()


# --- 2. API ë°ì´í„° í˜•ì‹ ì •ì˜ (Pydantic) ---
class PipelineRequest(BaseModel): # ì…ë ¥ ê°’
    storage_mode: str = Field(
        default=STORAGE_MODE,
        description="ìŠ¤í† ë¦¬ì§€ ëª¨ë“œ (ë¡œì»¬ ë˜ëŠ” S3)",
        example="local or s3"
    )
    query: str = Field(..., description="í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (í•„ìˆ˜)", example="ì„±ìˆ˜ë™ ì¹´í˜")
    latitude: Optional[float] = Field(None, description="ê²€ìƒ‰ ê¸°ì¤€ì  ìœ„ë„ (ì„ íƒ)", example=37.544)
    longitude: Optional[float] = Field(None, description="ê²€ìƒ‰ ê¸°ì¤€ì  ê²½ë„ (ì„ íƒ)", example=127.044)
    zoom_level: Optional[int] = Field(None, description="ì§€ë„ í™•ëŒ€ ë ˆë²¨(ê¸°ë³¸ ê°’ 15) (ì„ íƒ)", example=15) # [ì‹ ê·œ] zoom_level í•„ë“œ ì¶”ê°€
    show_browser: bool = Field(False, description="í¬ë¡¤ë§ ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ ì—¬ë¶€ (ë””ë²„ê¹…ìš©)")

class TargetPipelineRequest(BaseModel): # ì…ë ¥ ê°’
    storage_mode: str = Field(
        default=STORAGE_MODE,
        description="ìŠ¤í† ë¦¬ì§€ ëª¨ë“œ (ë¡œì»¬ ë˜ëŠ” S3)",
        example="local or s3"
    )
    # ê°€ê²Œ ì´ë¦„ê³¼ ì£¼ì†Œë¥¼ í•¨ê»˜ ë°›ìœ¼ë©´ ë” ì •í™•í•œ ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    query: str = Field(..., description="í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ (í•„ìˆ˜)", example="ì„±ìˆ˜ë™ ì¹´í˜")
    address: str = Field(..., description="í¬ë¡¤ë§í•  ê°€ê²Œì˜ ì •í™•í•œ ì£¼ì†Œ(í•„ìˆ˜)", example="ê°•ë‚¨êµ¬")
    latitude: Optional[float] = Field(None, description="ê²€ìƒ‰ ê¸°ì¤€ì  ìœ„ë„ (ì„ íƒ)", example=37.544)
    longitude: Optional[float] = Field(None, description="ê²€ìƒ‰ ê¸°ì¤€ì  ê²½ë„ (ì„ íƒ)", example=127.044)
    zoom_level: Optional[int] = Field(None, description="ì§€ë„ í™•ëŒ€ ë ˆë²¨(ê¸°ë³¸ ê°’ 15) (ì„ íƒ)", example=15) # [ì‹ ê·œ] zoom_level í•„ë“œ ì¶”ê°€
    show_browser: bool = Field(False, description="í¬ë¡¤ë§ ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ ì—¬ë¶€ (ë””ë²„ê¹…ìš©)")

class TaskResponse(BaseModel): # ì‘ì—… ì‘ë‹µ í˜•ì‹
    task_id: str
    message: str

class StatusResponse(BaseModel):
    task_id: str
    status: str
    request_details: Optional[Dict[str, Any]] = Field(None, description="ìµœì´ˆ ìš”ì²­ íŒŒë¼ë¯¸í„°")
    # â˜…â˜…â˜… ìƒì„¸ ì§„í–‰ ìƒí™©ì„ ìœ„í•œ í•„ë“œ ì¶”ê°€
    progress: Optional[Dict[str, str]] = Field(None, description="íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™©")
    result_path: Optional[str] = Field(None, description="[ë¡œì»¬ ëª¨ë“œ] ê²°ê³¼ íŒŒì¼ì´ ì €ì¥ëœ ë¡œì»¬ ê²½ë¡œ")
    result_url: Optional[str] = Field(None, description="[S3 ëª¨ë“œ] ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ì„ì‹œ URL")
    error: Optional[str] = None
    error: Optional[str] = None

# --- 3. í•µì‹¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
# ì¼ë°˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
def execute_pipeline_task(task_id: str, request: PipelineRequest, existing_ids: set):
    """ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¡œì§ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ìš©)"""
    output_dir = ""
    try:
        tasks_db[task_id] = {
            "status": "processing", # ì „ì²´ ìƒíƒœëŠ” 'processing'ìœ¼ë¡œ ìœ ì§€
            "request_details": request.model_dump(),
            "progress": {
                "ë„¤ì´ë²„ í¬ë¡¤ë§": "pending",
                "ì¹´ì¹´ì˜¤ í¬ë¡¤ë§": "pending",
                "ì ìˆ˜ ì‚°ì •": "pending",
                "ê²°ê³¼ ì €ì¥": "pending"
            }
        }
        print(f"[{task_id}] íŒŒì´í”„ë¼ì¸ ì‹œì‘: query='{request.query}'")
        
        # 1. Naver Crawling
        tasks_db[task_id]["progress"]["ë„¤ì´ë²„ í¬ë¡¤ë§"] = "running"
        naver_df = run_naver_crawling(
            search_query=request.query,
            latitude=request.latitude,
            longitude=request.longitude,
            headless_mode=(not request.show_browser),
            zoom_level=request.zoom_level,
            existing_naver_ids=existing_ids
        )
        if naver_df.empty: raise ValueError("ë„¤ì´ë²„ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        tasks_db[task_id]["progress"]["ë„¤ì´ë²„ í¬ë¡¤ë§"] = "completed"
        print(f"[{task_id}] ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ.")

        # 2. Kakao Crawling
        tasks_db[task_id]["progress"]["ì¹´ì¹´ì˜¤ í¬ë¡¤ë§"] = "running"
        kakao_df = run_kakao_crawling(
            input_df=naver_df,
            max_threads=config.get('num_threads', 3),
            headless=(not request.show_browser)
        )
        tasks_db[task_id]["progress"]["ì¹´ì¹´ì˜¤ í¬ë¡¤ë§"] = "completed"
        print(f"[{task_id}] ì¹´ì¹´ì˜¤ í¬ë¡¤ë§ ì™„ë£Œ.")

        # 3. Scoring
        tasks_db[task_id]["progress"]["ì ìˆ˜ ì‚°ì •"] = "running"
        final_list = run_scoring_pipeline(
            input_data=kakao_df.to_dict('records'),
            data_dir=config.get('data_dir', 'data')
        )
        if not final_list: raise ValueError("ì ìˆ˜ ì‚°ì • ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        tasks_db[task_id]["progress"]["ì ìˆ˜ ì‚°ì •"] = "completed"
        print(f"[{task_id}] ì ìˆ˜ ì‚°ì • ì™„ë£Œ.")

        final_df = pd.DataFrame(final_list)
        
        # --- ê³µí†µ íŒŒì¼ëª… ë° ê²½ë¡œ ìƒì„± ---
        now = datetime.now()
        timestamp = now.strftime('%Y%m%d_%H%M%S')
        safe_query_name = re.sub(r'[\\/*?:"<>|]', "", request.query)
        file_name = f"{safe_query_name}_{timestamp}_{task_id[:8]}_{len(final_df)}.json"
        
        tasks_db[task_id]["progress"]["ê²°ê³¼ ì €ì¥"] = "running" # ì €ì¥ ì‹œì‘

        # -- ìŠ¤í† ë¦¬ì§€ ëª¨ë“œì— ë”°ë¥¸ ê²°ê³¼ ì €ì¥ ë¶„ê¸° -- # 
        if request.storage_mode == "s3":
            s3_config = config['s3_config']
            s3_client = boto3.client('s3')
            date_path = now.strftime('%Y-%m/%Y-%m-%d')
            final_s3_key = f"{s3_config['output_results_prefix']}{date_path}/{file_name}"

            json_buffer = final_df.to_json(orient='records', force_ascii=False, indent=4)
            s3_client.put_object(Bucket=s3_config['bucket_name'], Key=final_s3_key, Body=json_buffer, ContentType='application/json')
            
            tasks_db[task_id].update({"status": "completed", "s3_key": final_s3_key})
            print(f"[{task_id}] S3ì— ê°œë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {final_s3_key}")

        # local ëª¨ë“œì¼ ê²½ìš°
        else:
            local_config = config['local_config']
            date_path = now.strftime(os.path.join('%Y-%m', '%Y-%m-%d'))
            output_path = os.path.join(local_config['output_dir'], date_path)
            os.makedirs(output_path, exist_ok=True)
            
            final_local_path = os.path.join(output_path, file_name)
            final_df.to_json(final_local_path, orient='records', force_ascii=False, indent=4)
            
            tasks_db[task_id].update({"status": "completed", "result_path": final_local_path})
            print(f"[{task_id}] ë¡œì»¬ì— ê°œë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {final_local_path}")

    except Exception as e:
        error_message = str(e)
        tasks_db[task_id].update({"status": "failed", "error": error_message})
        print(f"[{task_id}] íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {error_message}")
        if output_dir:
            error_log_path = os.path.join(output_dir, "error_log.txt")
            with open(error_log_path, 'w', encoding='utf-8') as f:
                f.write(f"Error: {error_message}\n\n")
                f.write(traceback.format_exc())

# íƒ€ê²Ÿ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜
def execute_target_pipeline_task(task_id: str, request: TargetPipelineRequest, existing_ids: set):
    """ë‹¨ì¼ íƒ€ê²Ÿ ë§¤ì¥ì— ëŒ€í•œ íŒŒì´í”„ë¼ì¸ ë¡œì§ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ìš©)"""
    output_dir = ""
    try:
        tasks_db[task_id] = {
            "status": "processing", # ì „ì²´ ìƒíƒœëŠ” 'processing'ìœ¼ë¡œ ìœ ì§€
            "request_details": request.model_dump(),
            "progress": {
                "ë„¤ì´ë²„ í¬ë¡¤ë§": "pending",
                "ì¹´ì¹´ì˜¤ í¬ë¡¤ë§": "pending",
                "ì ìˆ˜ ì‚°ì •": "pending",
                "ê²°ê³¼ ì €ì¥": "pending"
            }
        }
        print(f"[{task_id}] íƒ€ê²Ÿ íŒŒì´í”„ë¼ì¸ ì‹œì‘: search_query='{request.query}', address='{request.address}'")

        # 1. Target Naver Crawling
        tasks_db[task_id]["progress"]["íƒ€ê²Ÿ ë„¤ì´ë²„ í¬ë¡¤ë§"] = "running"
        # ìƒˆë¡œìš´ íƒ€ê²Ÿ í¬ë¡¤ëŸ¬ ì»¨íŠ¸ë¡¤ëŸ¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        naver_df = run_target_naver_crawling(
            search_query=request.query,
            address=request.address,
            latitude=request.latitude,
            longitude=request.longitude,
            headless_mode=(not request.show_browser),
            zoom_level=request.zoom_level,
            existing_naver_ids=existing_ids

        )
        if naver_df.empty: raise ValueError("íƒ€ê²Ÿ ë„¤ì´ë²„ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        tasks_db[task_id]["progress"]["íƒ€ê²Ÿ ë„¤ì´ë²„ í¬ë¡¤ë§"] = "completed"
        print(f"[{task_id}] íƒ€ê²Ÿ ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ.")

        # 2. Kakao Crawling
        tasks_db[task_id]["progress"]["ì¹´ì¹´ì˜¤ í¬ë¡¤ë§"] = "running"
        kakao_df = run_kakao_crawling(
            input_df=naver_df,
            max_threads=config.get('num_threads', 3),
            headless=(not request.show_browser)
        )
        tasks_db[task_id]["progress"]["ì¹´ì¹´ì˜¤ í¬ë¡¤ë§"] = "completed"
        print(f"[{task_id}] ì¹´ì¹´ì˜¤ í¬ë¡¤ë§ ì™„ë£Œ.")

        # 3. Scoring
        tasks_db[task_id]["progress"]["ì ìˆ˜ ì‚°ì •"] = "running"
        final_list = run_scoring_pipeline(
            input_data=kakao_df.to_dict('records'),
            data_dir=config.get('data_dir', 'data')
        )
        if not final_list: raise ValueError("ì ìˆ˜ ì‚°ì • ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        tasks_db[task_id]["progress"]["ì ìˆ˜ ì‚°ì •"] = "completed"
        print(f"[{task_id}] ì ìˆ˜ ì‚°ì • ì™„ë£Œ.")

        final_df = pd.DataFrame(final_list)
        
        # --- ê³µí†µ íŒŒì¼ëª… ë° ê²½ë¡œ ìƒì„± ---
        now = datetime.now()
        timestamp = now.strftime('%Y%m%d_%H%M%S')
        safe_query_name = re.sub(r'[\\/*?:"<>|]', "", request.query)
        file_name = f"{safe_query_name}_{timestamp}_{task_id[:8]}_{len(final_df)}.json"
        
        tasks_db[task_id]["progress"]["ê²°ê³¼ ì €ì¥"] = "running" # ì €ì¥ ì‹œì‘

        # -- ìŠ¤í† ë¦¬ì§€ ëª¨ë“œì— ë”°ë¥¸ ê²°ê³¼ ì €ì¥ ë¶„ê¸° -- # 
        if request.storage_mode == "s3":
            s3_config = config['s3_config']
            s3_client = boto3.client('s3')
            date_path = now.strftime('%Y-%m/%Y-%m-%d')
            final_s3_key = f"{s3_config['output_results_prefix']}{date_path}/{file_name}"

            json_buffer = final_df.to_json(orient='records', force_ascii=False, indent=4)
            s3_client.put_object(Bucket=s3_config['bucket_name'], Key=final_s3_key, Body=json_buffer, ContentType='application/json')
            
            tasks_db[task_id].update({"status": "completed", "s3_key": final_s3_key})
            print(f"[{task_id}] S3ì— ê°œë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {final_s3_key}")

        # local ëª¨ë“œì¼ ê²½ìš°
        else:
            local_config = config['local_config']
            date_path = now.strftime(os.path.join('%Y-%m', '%Y-%m-%d'))
            output_path = os.path.join(local_config['output_dir'], date_path)
            os.makedirs(output_path, exist_ok=True)
            
            final_local_path = os.path.join(output_path, file_name)
            final_df.to_json(final_local_path, orient='records', force_ascii=False, indent=4)
            
            tasks_db[task_id].update({"status": "completed", "result_path": final_local_path})
            print(f"[{task_id}] ë¡œì»¬ì— ê°œë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {final_local_path}")

    except Exception as e:
        error_message = str(e)
        tasks_db[task_id].update({"status": "failed", "error": error_message})
        print(f"[{task_id}] íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {error_message}")
        if output_dir:
            error_log_path = os.path.join(output_dir, "error_log.txt")
            with open(error_log_path, 'w', encoding='utf-8') as f:
                f.write(f"Error: {error_message}\n\n")
                f.write(traceback.format_exc())


# --- 4. ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰ë  ì´ë²¤íŠ¸ ---
def clean_firefox_cache():
    """Firefox ê´€ë ¨ ìºì‹œ íŒŒì¼ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    try:
        print("ğŸ§¹ /tmp ë‚´ Firefox ìºì‹œ íŒŒì¼ ì •ë¦¬ ì‹œë„...")
        if sys.platform != "win32": # ìœˆë„ìš°ê°€ ì•„ë‹ ê²½ìš°ì—ë§Œ ì‹¤í–‰
            subprocess.run("rm -rf /tmp/rust_mozprofile* /tmp/Temp-*profile /tmp/geckodriver*", shell=True, check=False)
            print("âœ… ìºì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ.")
        else:
            print("- ìœˆë„ìš° í™˜ê²½ì—ì„œëŠ” ìºì‹œ ì •ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ìºì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def setup_api_key():
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("ê²½ê³ : GOOGLE_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Scoring ë‹¨ê³„ê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    genai.configure(api_key=api_key)
    print("âœ… Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")


@app.on_event("startup")
def on_startup():
    print(f"ğŸš€ API ì„œë²„ ì‹œì‘... (ìŠ¤í† ë¦¬ì§€ ëª¨ë“œ: {STORAGE_MODE.upper()})")
    setup_api_key()
    clean_firefox_cache()

# --- 5. API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„ --- # ì´ê±° ì–´ë–»ê²Œ post ë„˜ê²¨ì„œ ê°’ ë°›ì„ ì§€ ë‹¤ì‹œ ì •í•˜ê¸°
# ì¼ë°˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ------------------------------
@app.post("/pipeline/run", response_model=TaskResponse, status_code=202)
async def start_pipeline_endpoint(request: PipelineRequest, background_tasks: BackgroundTasks):
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ìš”ì²­í•˜ê³  ì¦‰ì‹œ ì‘ì—… IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    task_id = str(uuid.uuid4())
    tasks_db[task_id] = {"status": "pending"}

    print(f"[{task_id}] ê¸°ì¡´ ë§ˆìŠ¤í„° ë°ì´í„°ì—ì„œ naver_id ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
    existing_ids = load_ids_from_master_data(request.storage_mode, config)

    background_tasks.add_task(execute_pipeline_task, task_id, request, existing_ids)

    return {"task_id": task_id, "message": "íŒŒì´í”„ë¼ì¸ ì‘ì—…ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤. '/pipeline/status/{task_id}'ë¡œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."}

# íƒ€ê²Ÿ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ ------------------------------
@app.post("/pipeline/target-run", response_model=TaskResponse, status_code=202)
async def start_target_pipeline_endpoint(request: TargetPipelineRequest, background_tasks: BackgroundTasks):
    """(ì‹ ê·œ) ì£¼ì†Œ ì •ë³´ë¥¼ í™œìš©í•œ ë‹¨ì¼ íƒ€ê²Ÿ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ìš”ì²­í•©ë‹ˆë‹¤."""
    task_id = str(uuid.uuid4())
    tasks_db[task_id] = {"status": "pending"}
    
    # íƒ€ê²Ÿ í¬ë¡¤ë§ì€ ë³´í†µ ì¤‘ë³µ ID ì²´í¬ê°€ ëœ ì¤‘ìš”í•˜ì§€ë§Œ, í˜¸í™˜ì„±ì„ ìœ„í•´ ì „ë‹¬
    existing_ids = load_ids_from_master_data(request.storage_mode, config)
    
    # íƒ€ê²Ÿ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜ë¥¼ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì¶”ê°€
    background_tasks.add_task(execute_target_pipeline_task, task_id, request, existing_ids)

    return {"task_id": task_id, "message": "íƒ€ê²Ÿ íŒŒì´í”„ë¼ì¸ ì‘ì—…ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤. '/pipeline/status/{task_id}'ë¡œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."}

# íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸ í•¨ìˆ˜ ------------------------------
@app.get("/pipelines/status/{task_id}", response_model=StatusResponse)
async def get_pipeline_status(task_id: str):
    """ì£¼ì–´ì§„ task_idì— ëŒ€í•œ ì‘ì—… ìƒíƒœì™€ ê²°ê³¼ ê²½ë¡œ/URLì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    task = tasks_db.get(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="í•´ë‹¹ task_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    response_data = deepcopy(task) # ì›ë³¸ DB ìˆ˜ì •ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê¹Šì€ ë³µì‚¬
    response_data['task_id'] = task_id

    # S3 ëª¨ë“œì´ê³  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆì„ ê²½ìš°, ì„ì‹œ URL(Presigned URL) ìƒì„±
    if STORAGE_MODE == 's3' and task.get("status") == "completed" and "s3_key" in task:
        s3_config = config['s3_config']
        s3_client = boto3.client('s3')
        try:
            url = s3_client.generate_presigned_url(
                'get_object',
                Params={'Bucket': s3_config['bucket_name'], 'Key': task['s3_key']},
                ExpiresIn=3600  # 1ì‹œê°„ ë™ì•ˆ ìœ íš¨í•œ URL
            )
            response_data['result_url'] = url
        except Exception as e:
            response_data['error'] = f"S3 URL ìƒì„± ì‹¤íŒ¨: {e}"

    return response_data

# config í™•ì¸ í•¨ìˆ˜ ------------------------------
@app.get("/config", response_model=dict)
async def get_config():
    """ì„œë²„ì— ë¡œë“œëœ ì „ì²´ ì„¤ì •ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    # ë¯¼ê° ì •ë³´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜í•´ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    return config


# --- 6. ë°ì´í„° í†µí•© ìˆ˜ë™ ì‹¤í–‰ API ---
def consolidation_task_wrapper():
    """ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ í›„ ì ê¸ˆ í”Œë˜ê·¸ë¥¼ í•´ì œí•˜ëŠ” ë˜í¼ í•¨ìˆ˜."""
    global CONSOLIDATION_IN_PROGRESS
    try:
        run_consolidation_job()
    finally:
        CONSOLIDATION_IN_PROGRESS = False
        print("ë°ì´í„° í†µí•© ì‘ì—… ì™„ë£Œ. ì´ì œ ë‹¤ìŒ í†µí•© ìš”ì²­ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ë°ì´í„° í†µí•© ìˆ˜ë™ ì‹¤í–‰ API ------------------------------
@app.post("/admin/consolidation", response_model=TaskResponse, status_code=202)
async def trigger_consolidation_endpoint(background_tasks: BackgroundTasks):
    """ë°ì´í„° í†µí•© ë°°ì¹˜ ì‘ì—…ì„ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰ì‹œí‚µë‹ˆë‹¤. (ê´€ë¦¬ììš©)"""
    #  í•¨ìˆ˜ ë‚´ì— 'global' í‚¤ì›Œë“œ ì„ ì–¸ ì¶”ê°€ 
    global CONSOLIDATION_IN_PROGRESS
    
    if CONSOLIDATION_IN_PROGRESS:
        raise HTTPException(
            status_code=409,
            detail="ë°ì´í„° í†µí•© ì‘ì—…ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )

    CONSOLIDATION_IN_PROGRESS = True
    print("ê´€ë¦¬ì ìš”ì²­ìœ¼ë¡œ ë°ì´í„° í†µí•© ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.")
    background_tasks.add_task(consolidation_task_wrapper)
    
    return {"task_id": "consolidation_job", "message": "ë°ì´í„° í†µí•© ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."}
